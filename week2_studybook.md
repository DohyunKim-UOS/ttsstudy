# Week.2 다변수 정규 분포의 행렬 표현

|발표자|자습서|과제제작|
|-----|-----|------|
|홍정민|김도현|이혜성|

2주차 자습서입니다. 2주차의 주제는 다변수 정규 분포와 행렬 표현입니다.

## Part.1 MVN과 공분산 행렬의 수식적 정의 및 성질

### 1.1 MVN의 도입과 사용 이유
다들 전공 수업을 통해 일변수 정규분포에 대해서는 충분히 공부하셨을 것이라 생각합니다. 

$$f(x;\mu, \sigma^2) = \frac {1} {\sqrt{2 \pi \sigma^2 }} e^{-\frac {(x-\mu)^2}{2 \sigma^2}} $$

위 식은 단일한 확률 변수 $X$에 대한 정규분포의 pdf입니다. 이때 $X$는 (확률) 변수, $\mu, \sigma^2$은 파라미터입니다. 변수는 함수의 출력값인 $y$를 결정짓는 요소, 파라미터는 함수 자체를 결정하는 요소라고 생각하시면 될 것 같아요. 

이때 확률 변수 $X$는 **function**이자 분포를 결정짓는 **feature**입니다. 예를 들어 성적이 정규분포를 따른다면, 이때 학생 A, 학생 B, ... , 학생 C는 각각 표본 공간을 구성하는 원소들입니다. 하지만 이 학생들의 성적을 분석하기 위한 변수로 학생 A ...를 그대로 활용할 수 있을까요??

어려울 겁니다. 우리는 분포의 함수인 pdf를 이용해 분석하고자 하는데 함수의 인풋이 '학생 A' 또는 '학생 B'라면 거의 함수로서의 기능을 할 수가 없겠죠??

왜냐하면 분포의 pdf함수를 구성하는 초월함수 파트($e^x$)등은 인풋으로 실수를 요구하기 때문입니다.

따라서 우리는 확률 분포 함수 pdf의 인풋이 '학생 A'와 같은 단일 요소가 아닌 실수이기를 원합니다. 따라서 우리는 정의역 표본 공간으로, 치역은 실수인 하나의 함수를 만들어낼 수 있겠습니다. 앞선 예시에서 학생 A를 1로 대응, 학생 B를 2로 대응... 이런 식이죠! 이렇게 대응시키는 함수가 **확률 변수**이고, 이러한 함수 관계(규칙)이 드러나기 때문에 우리는 확률 변수를 **함수**라고 부르는 것이죠!

$$X:표본 공간 \rightarrow \mathbb{R}$$

이제 앞선 예시에서는 확률 변수 X가 각각의 학생이 되고 각각의 **학생이 분포의 값을 결정짓는 fearure입니다**. 어떤 학생이냐에 따라 성적이 결정되는 것이죠!

*그렇다면 음성 데이터도 위처럼 단일 변수를 갖는 정규 분포를 이용해 분포를 나타낼 수 있을까요?*

어렵습니다! "안녕하세요! 저는 안재현입니다. 저는 오늘 수업을 안 갔어요!"라는 문장을 생각해 볼까요?? 이 문장에는 '안'이라는 음절이 3번 등장합니다. 하지만 이 3번의 '안'은 모두 억양과 음의 높낮이가 모두 다릅니다. (읽어보세요^^) 

문장의 처음에 오냐, 중간에 오냐, 또 기쁘냐 슬프냐에 따라서 발음과 높낮이가 달라집니다. 즉!! 저 '안'의 음성을 결정짓는 feature는 한 두개가 아니기 때문에 음성을 일변수 정규분포로는 결정지을 수 없는 것입니다. 따라서 우리는 여러개의 feature를 바탕으로 분포값이 결정되는 MVN(Multi Variable Normal Distribution)을 도입할 필요가 있습니다.


### 1.2 MVN 수식 유도

$$f(x;\mu, \sigma^2) = \frac {1} {\sqrt{2 \pi \sigma^2 }} e^{-\frac {(x-\mu)^2}{2 \sigma^2}} $$
이 식부터 살펴봅시다. 이 식에서 expoinential 안에 있는 $-\frac{(x-\mu)^2} {2 \sigma^2}$은 사실 분포를 고려한 거리입니다. 분자의 $(x-\mu)^2$은 단순한 유클리드 거리(1차원 상)를 의미하고, 분모의 $2\sigma^2$은 분포에 따라 달라집니다. 따라서 분포를 고려한 거리인 셈이죠. (앞에 붙어있는 계수는 정규화계수로, pdf의 적분값이 1이 되도록 하기 위해 곱한 값입니다.)

$-\frac{(x-\mu)^2} {2 \sigma^2}$를 고차원으로 바꿔가며 MVN으로 확장해 봅시다!

$$-\frac{(x-\mu)^2} {2 \sigma^2} = -\frac{1}{2} \times (x-\mu) \times \sigma^{-2} \times (x-\mu)$$

에 대해 $\mathbf{x}=\begin{pmatrix} x1 \\ x2 \end{pmatrix}, \mathbf{\mu} =\begin{pmatrix} \mu1 \\ \mu2 \end{pmatrix} $라 하고 $\mathbf{\sum}$은 공분산 행렬이라 하면, 벡터의 내적 표현(제곱)에 의해 $-\frac{1}{2} \times (x-\mu) \times \sigma^{-2} \times (x-\mu)$는 아래와 같이 표현됩니다.

$$-\frac{1}{2} \times (x-\mu) \times \sigma^{-2} \times (x-\mu) = -\frac{1} {2} (\mathbf{x}-\mathbf{\mu})^T {\mathbf{\sum}}^{-2}(\mathbf{x}-\mathbf{\mu})$$

가 되며, 분포의 pdf 적분값이 1이 되도록 계수를 구하면 $\frac {1} {(2\pi)^{n/2}|\mathbf{\sum}|^{1/2}}$이므로, 최종적으로 MVN의 pdf는

$$f(\mathbf{x}; \mathbf{\mu}, \mathbf{\sigma^2}) = \frac {1} {(2\pi)^{n/2} |\mathbf{\sum}|^{1/2}}\exp{\left[-\frac{1}{2}(\mathbf{x} - \mathbf{\mu})^T \mathbf{\sum}^{-2}(\mathbf{x}-\mathbf{\mu})\right]}$$

편의상 이후 벡터들에 대한 bold 표현을 생략하겠습니다... bold가 아니더라도 적절히 벡터라고 이해해 주시면 감사하겠습니다!!

### 1.3 공분산 행렬의 성질
공분산 행렬: 변수들간의 공분산(퍼짐 정도, 관계성)을 담은 행렬입니다. $\sigma^2$과 유사한 역할을 수행합니다. 공분산행렬은 아래와 같은 성질을 갖습니다.

* 대칭성: 공분한행렬은 정의상 항상 대칭행렬입니다! ($\sum = \sum^T$)

* 양의 준정부호성: 선형대수 시간에 배웠던 반양정치행렬과 같은 개념입니다. 양정치행렬이 정부호행렬이고, 반양정치행렬이 준정부호행렬입니다. 아래에서 조금 더 꼼꼼히 살펴보겠습니다!

양의 준정부호성의 정의부터 살펴볼까요??

정방행렬 $A$가 아래 조건을 만족시키면 행렬 $A$는 준정부호행렬이다.

$$\mathbf{x}^T A \mathbf{x} \geqslant 0, \forall \mathbf{x}$$

직관적으로 살펴보면, 어떠한 $\mathbf{x}$도 $A$를 음으로 만들어내지 못한다는 것입니다.

일차원 상에서의 예시를 들어보면 분산을 살펴볼 수 있겠죠! 분산의 정의는 아래와 같습니다.

$$\frac {1} {n} \sum_{i=1} ^{n} {(\mathbf{X}_i - \mu)^2} = \sigma^2$$

사실 우리가 너무나도 익숙한 이 분산은 확률 변수가 $X$ 하나일 때의 분산입니다. 그리고 맨날 익숙하게 배웠던 공분산 행렬은 확률 변수가 여러개인 joint 상황에 대한 분산의 역할을 수행하는 것이죠.

일차원 수직선 상에서 $\sigma^2$이 항상 0보다 크거나 같듯, 공분산 행렬도 항상 0보다 크거나 같아야 하지 않을까요?? 근데 여기에는 약간의 문제가 있습니다. 

일변수 상황에서의 분산은 상수이기 때문에 0보다 크거나 같다는 것이 우리에게 직관적으로 너무나 와닿습니다. 하지만 공분산 행렬 $\mathbf{\sum}$은 행렬인걸료??

그래서 우리는 공분산 행렬을 임의의 1차원 벡터로 사영했을 때(이게 사실 1차원 상수로 만드는 과정이죠! $\sigma^2\prime$ 이라고 보시면 될 것 같아요! 우리가 일차원에서 다루는 $\sigma^2$은 축이 $X$ 축인 것이고, $\sigma^2\prime$은 임의의 일차원 축에 사영한 것이라고 생각하면 될 것 같네요!) 

이런식으로 사영을 이용해 차원을 꾸겨 공분산 행렬을 1차원으로 바라보면 항상 양수여야 합니다!

따라서 임의의 벡터 $\mathbf{v}$에 대해 $x$를 $v$에 스칼라사영한 벡터를 $Y$라 하면, 정의에 의해 

$$Y=\mathbf{v}^T x$$
$$\begin{align} Var(\mathbf{Y}) &= Var(\mathbf{v}^T x) \\
& = \mathbf{v}^T Var(x) (\mathbf{v}^T)^T\quad&(\because Var(Ax) = A^T Var(x) A)\\
& = \mathbf{v}^T Var(x) \mathbf{v} &(\because (v^T)^T = v)\\
& = \mathbf{v}^T \sum \mathbf{v} \end{align}$$

이때 임의의 1차원 벡터 $\mathbf{v}$로 사영한 $Y$의 분산은 항상 0보다 커야하므로, $Var(Y) = \mathbf{v}^T \mathbf{\sum} \mathbf{v} \geqslant 0$여야 하고 양의 준정부호 행렬의 정의에 따라 공분산 행렬 $\mathbf{\sum}$은 항상 양의 준정부호 행렬이 됩니다.


## Part.2 고윳값 분해의 정의와 공분산 행렬에의 적용

### 0. Introduction

수학교육론에 따르면 우리가 초등학교~고등학교에서 배우는 수학은 주로 해석학 베이스입니다. 해석학이란 말그대로 해석을 하는 학문인데, 이때 해석의 대상은 무엇일까요?? 바로 **함수**입니다. 우리는 어떠한 잘 모르는 함수를 효율적으로 해석해내기 위해, 함숫값도 구해보고 극한값도 구해보고, 미분도 해보고, 적분도 합니다. 극한, 연속, 미분, 적분 등의 도구는 함수를 해석하기 위한 것들이죠!

찬찬히 생각해보면, 여러분 고등학교때 수학I을 제외한다면 전부 1단원은 극한과 연속, 2단원은 미분, 3단원은 적분이었을 거예요! 그 대상이 되는 함수가 다항함수이냐 아니면 수열/초월함수이냐의 차이일 뿐이죠! 왜 고등수학이 해석학 베이스라고 했는지 아시겠죠??

함수는 일종의 규칙입니다! 함수에 수를 넣으면 특정한 규칙을 수행하고 값을 내뱉죠. "2를 곱한다"가 규칙이라면 2를 넣으면 4가 나오고, 3을 나오면 6이 나오는 일종의 함수 $f(x) = 2x$가 탄생하는 것이죠.

이제 우리는 대학에 와서 선형대수를 배우고 미분기하학을 배우고 하다보면 해석학에서 약간 벗어나는 **변환(Transformation)** 이라는 개념을 배웁니다! 변환을 처음 보면 용어도 낯설고 뭔지도 모르니 그냥 외워버리죠... 개인적으로 고등학생때 해석학만 주구장창 할게 아니라 선형대수도 배우고, 기하학도 배워야 한다고 생각합니다.

각설하고, 함수가 해석학에서의 대응 규칙이였다면, 변환은 행렬에서의 대응 규칙(함수)라고 생각하시면 편할 것 같아요! 임의의 벡터 $\mathbf{x}$를 함수에 넣어서 $\mathbf{y}$를 얻었다면, $\mathbf{y} = f(\mathbf{x})$라면, 이때 $f$가 변환입니다! 행렬에서는 변환을 **행렬곱**을 통해 진행하게 되는 것이죠! 

예를 들어, 벡터 $\mathbf{x} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$에 $A = \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix}$를 곱한 $A\mathbf{x}=\begin{pmatrix} 6 \\ 8 \end{pmatrix} = \mathbf{y}$라 하면 $A$는 2차원상의 변환 행렬인 것이죠! 너무 쉽죠?? 특히 값을 보시면 아시겠지만, 행렬 $A$는 이차원 벡터를 스칼라 두 배 하는 일차 변환 행렬입니다! 수학II를 열심히 수강하신 분들이라면 아시겠죠 ㅎㅎ

일차변환, 이차변환, 다차원 변환에 대해서도 설명할 기회가 있으면 좋겠습니다! 여기서 설명하기에는 너무 길기도 하고 주제와도 벗어나니 직접 공부해 보시길 바랍니다.

### 1. 고윳값과 고유벡터, 그리고 고윳값분해

![변환](https://datascienceschool.net/_images/e6fc0376a630ec66da0871c67c661eb61423953fc7faa9207bd91fb40c32b18e.png)

$g1$이라는 원래의 벡터가 있고, 일종의 변환 행렬 $A$를 곱하여 $g2$라는 행렬을 얻었다고 해볼게요! 그럼 $A$는 하나의 변환 행렬이죠.

이번에는 $g1$에 $B$라는 변환 행렬을 곱해, $\mathbf{x}$라는 벡터를 만들었다고 해볼게요!! 

두 operation의 차이를 아시겠나요...?? 맞습니다. $g2$와는 다르게 $x$는 원래 벡터인 $g1$과 방향이 같고 크기만 다릅니다. 되게 특수한 경우인 것이죠.

그렇다면 임의의 변환 행렬 $A$에 대해, 방향은 바꾸지 않고 스케일만을 바꾸는 벡터 $\mathbf{x}$를 찾아낼 수 있을까요??

즉, 다시 말해 $A\mathbf{x} = \lambda\mathbf{x}$를 만족하는 $\mathbf{x}$와 $\lambda$를 찾을 수 있을까요??

***네 가능합니다*** 그게 바로 고윳값, 고유벡터, 고윳값 분해의 정의입니다!!

어떠한 행렬 $A$의 고윳값 벡터는, 행렬 A를 통해 벡터 $\mathbf{x}$의 변환을 시행했을 때 나오는 결과 벡터가 $\mathbf{x}$와 방향은 같고 스케일만이 다른 벡터입니다. 이때의 스케일 $\lambda$가 고윳값인 것이죠!! 너무 쉽죠??

그럼 어떻게 구할까요?? $A\mathbf{x} = \lambda \mathbf{x}$를 정리하면

$$\begin{align} A\mathbf{x} - \lambda \mathbf{x} = 0 \\
(A-\lambda I)\mathbf{x} = 0\end {align}$$

수학II에서 배웠다시피 해당 시스템이 자명해를 갖지 않기 위해서는 $\det(A-\lambda I) \neq 0$이어야 합니다!

예를 들어 $A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$ 라면, 위 조건을 적용하여, $A - \lambda I = \begin{pmatrix} 2-\lambda & 1 \\ 1 & 2 - \lambda \end{pmatrix}$이고, $\det(A-\lambda I) = \lambda^2 -4\lambda +3 = 0$,  $ \quad  \quad \therefore \lambda=1 \quad or \quad  \lambda=3$

따라서 행렬 $A$의 고윳값은 1 또는 3이고, 각각의 고윳값일 때의 고유 벡터는, $\mathbf{x}_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$, $\mathbf{x}_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$ 입니다. 걍 대입해서 구하시면 돼요.

기하학적으로는, 두 벡터 $\mathbf{x}_1 \ \mathbf{x}_2$에 대해서는 행렬 $A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$로 변환을 시행하더라도 각각 1배, 3배가 될 뿐, 방향이 변하지 않습니다. 

그럼 잠깐 앞서 예시로 들어드렸던, 스케일 변환 벡터 $A = \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix}$를 같은 방법으로 한 번 살펴볼까요?? $A- \lambda I = \begin{pmatrix} 2-\lambda & 0 \\ 0 & 2- \lambda \end{pmatrix}$ 이고, 해당 행렬의 determinant를 구해보면, $(2-\lambda)^2 = 0$이어야 하므로, $\lambda = 2$, 직접 대입해 보면, $\begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \mathbf{x} = 2 \mathbf{x}$, $x = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$라 하면, $2x_1 = 2x_1$, $2x_2 = 2x_2$여서 모든 $\mathbf{x}$에 대해서 성립합니다! 모든 벡터가 고유벡터인 것이죠! (고윳값은 2니까 항상 두 배) 행렬 $A$는 벡터를 두 배 하도록 만든 변환 행렬이기 때문이죠.

그럼 이제 본격적으로 **고윳값 분해**를 알아봅시다. 

어떤 정방 행렬 $A \in \mathbb{R}^{n\times n}$에 대해 고윳값 $\lambda_1, \cdots,\lambda_n$이라고 하고 각각에 대응되는 고윳값 벡터들을 $\mathbf{v}_1, \cdots , \mathbf{v}_n$이라고 할 때, 

* $\mathbf{V} = [\mathbf{v}_1, \cdots, \mathbf{v}_n]$: 고유벡터들을 열벡터로 모든 행렬
* $\Lambda = diag(\lambda_1, \cdots , \lambda_n)$: 대응하는 고윳값들을 대각에 둔 행렬
* $A\mathbf{v}_i = \lambda_i \mathbf{v} _i$

위처럼 행렬 $A$를 우항처럼 쪼개는 것이 고윳값 분해입니다. 따라서 분해가 가능하려면, 반드시 $n$개의 고윳값을 가져야겠죠.

공분산 행렬은 $\sum = Q \Lambda Q^-1$으로 고윳값 분해할 수 있습니다. 이는 매우 중요한 의미를 갖습니다. 위에서 설명한 것처럼, 고유벡터는 행렬 A를 이용해 변환했을 때 스케일만을 바꾸는 경우이며 고윳값은 그 방향으로의 스케일을 의미합니다! 따라서 공분산 행렬을 고윳값 분해하여 살펴보면 그 방향으로 데이터가 어느 방향으로 퍼져 있는지 알 수 있는 것이죠!(고윳값과 고유 벡터의 정의에 의해)

### 2. 고윳값과 고유 벡터를 이용한 퍼짐성

> 공분산 행렬을 고윳값 분해하여 살펴보면 그 방향으로 데이터가 어느 방향으로 퍼져 있는지 알 수 있는 것이죠!(고윳값과 고유 벡터의 정의에 의해)

방금 설명드린 내용이에요! 예를 들어 타원 모양의 분포를 갖는 데이터를 생각해 봅시다! 

![타원 분포](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTu-go65AM5X9wKzufOfb0-alY1uB-MbK5JhA&s_)

저런 경우 공분산 행렬의 Rank에 의해 고윳값 두 개를 갖고, 고유 벡터 두 개를 갖습니다. 그리고 각각의 고유 벡터의 방향은 타원의 두 축이 됩니다! 따라서 해당 축의 방향으로 데이터가 얼마나 퍼져 있는지 분석할 수 있는 것이죠. 따라서

* 퍼짐의 방향: 타원의 주축 방향 = 고유 벡터의 방향 

* 퍼짐의 정도: 타원 주축의 길이 = 고윳값 = 타원 축으로 데이터들을 모두 사영했을 때의 분산

이 됩니다. 고윳값과 고유벡터의 기하학적 그리고 직관적 의미를 음미하며, 각 축으로의 사영의 의미, 그리고 각 축 방향으로의 데이터 퍼짐 정도의 의미를 꼼꼼히 음미해 보시길 바랍니다!


## Part.3 아핀 변환 및 백색화

### 1. 아핀 변환(Affine Transformation)

여러분! 기계학습 배우고 딥러닝 배우다 보면, Affine Layer라는 용어가 등장하지 않나요? 한 층을 통과한 Hidden Layer는

$$\mathbf{h} = f(\mathbf{W}x + b)$$

잖아요?? ($\mathbf{W}$: 가중치 벡터, $f$: Activation Function, $b$: Bias) 이때 $Wx+b$ 연산을 Affine 연산이라 부르고, 해당 연산을 수행하는 Fully Connected Layer를 Affine 계층이라고 부르는 겁니다!

즉, 다시 말해서 Affine 연산은 $Y=WX+b$로 나타나는, 선형 변환 이후, $b$만큼의 평행이동을 거치는 변환을 일컫습니다.

일차적으로 진행하는 선형 변환은 모양을 바꿉니다. 우리가 수학II에서 배운 스케일링, 층밀림 등이 이에 해당합니다.

그 다음 진행하는 평행 이동은 중심의 위치만을 바꿉니다.

MVN에는 독특한 특징이 있습니다! 바로 어떠한 MVN이든 Affine 변환 해도 그대로 MVN이라는 것이죠! 새로운 MVN이 만들어지는 것입니다.

수식적으로 표현하면, 

$$x \sim N(\mu, \sum) \Rightarrow Ax+b \sim N(A\mu, A\sum A^T)$$

proof)

$\text{ETS}: M_Y (t) = \exp\left( t^T (A\mu + b ) + \frac {1} {2} t^T (A \sum A^T ) t \right)$

$$M_X (t) = \mathbb{E}\left[ e^{t^TX} \right]= \exp(t^T \mu + \frac {1} {2} t^T \sum t),\quad t \in \mathbb{R}^d$$

$$M_Y (t) = \mathbb{E} \left[ e^{t^T Y}\right] = \mathbb{E} \left[ e^{t^T (AX+b)} \right] = e^{t^T b} \mathbb{E} \left[ e^{(A^T t)^T X} \right] = e^{t^T b} M_X(A^T t).$$

따라서,

$$M_Y (t) = \exp\left(t^T b + (A^T t)^T \mu + \frac {1} {2} (A^T t)^T \sum (A^T t)\right) \blacksquare$$



### 2. 샘플링에 아핀 변환 적용하기

우리는 종종 특정 분포에서 여러 샘플들을 추출해야 할 일이 생깁니다. 예를 들어 $N(\mathbf{\mu}, \mathbf{\sum})$에서 샘플 $x$를 추출하는 경우를 생각해 봅시다. 조금 까다롭죠??

어떻게 하면 쉽게 뽑을 수 있을까요?? 만약 매우 단순한 표준 정규 분포인 $N(0, I)$에서 추출한 $z$표본을 복원하여 $x$를 만들어낼 수 있다면 정말 좋겠죠???

우선 $X\sim N(\mathbf{\mu}, \mathbf{\sum})$으로부터 $Y \sim N(0, I)$로 만드는 과정도 아핀과정입니다!! 타원 모양의 분포를 갖는 $X$ 데이터를 일차 변환을 이용해 원으로 만들고($W$를 곱함), 이후 평행이동 하면 ($Y=WX+b$로 표현되고) 해당 변환은 아핀변환입니다! 그러니, 당연하게도 결과도 정규분포인 것이죠.

그럼 단순한 표준 정규분포 $Y$로부터 추출한 샘플 $z$를 $x$로 변환하는 과정을 알아봅시다!

우선 확률변수 $X=AZ+b$라고 해보겠습니다. $Z$는 위에서 $Y$와 같습니다. (표준정규분포). 이제 표준 정규 분포에서 샘플링한 $z$에 $A$를 곱하고 $b$를 더하는 아핀 변환을 통해 $X$에서 샘플링한 $x$를 만들어낼 수 있습니다!!

즉, $x = Az + b \quad \cdots \quad (a)$인 것이죠. 우리는 $A$와 $b$를 찾아야 합니다.

따라서 정규분포 기댓값의 선형성과, 분산 성질에 의해 $X \sim N(A\mu_z + b, A\sum_z A^T)$가 성립합니다! 원래 $X \sim N(\mu, \sum)$이었으므로,

$$ \begin{cases} A\mu_z + b = 0 + b = \mu \Rightarrow b = \mu \\ A \sum_z A^T = A\times I \times A^T = AA^T = \sum \Rightarrow \text{We need to find A that satisfies}\ AA^T = \sum \end{cases} $$

어렵지 않게 $b=\mu$는 찾을 수 있습니다. 이제 우리는 분산을 일치시키기 위해 $AA^T = \sum$을 만족하는 $A$를 찾아야 합니다! 

이때 쓸 수 있는 분해가 바로 촐레스키 분해!! 촐레스키 분해란, 

$$\exists L \in \mathbb{R}^{n \times n} \quad s.t. \ LL^T = A,\ \text{L은 하삼각행렬, A는 양의 정부호 행렬}$$

다만 행렬이 촐레스키 분해 되기 위해서는 반드시 대상이 되는 행렬이 양정치행렬(양의 정부호 행렬)이어야 합니다!!

앞서 우리가 수식 (1) ~ (4)를 통해 공분한 행렬은 항상 양의 정부호성을 갖기 때문에 공분산 행렬은 반드시 하삼각행렬과 하삼각행렬의 전치행렬 사이 곱으로 표현됩니다!!

즉, $\sum = L L^T$를 만족하는 $L$이 항상 존재한다는 것이죠. 따라서 $AA^T = \sum$을 만족하는 $A$는 촐레스키 분해를 이용해 구한 $L$이 됩니다.

따라서 식 (a)는 $x = \mu + Lz$가 되는 것입니다!!!!!!!

> $X$로부터의 샘플을 만들기 위해 $\rightarrow$ 간단한 $Z$에서 샘플링하여 $z$를 구하고 $\rightarrow$ $x=\mu+Lz$를 이용해 $X$로부터의 샘플 $x$를 구한다!

> 추가로 공부했으면 좋겠는 내용: 일차변환이란?? 그렇다면 일차변환이 아닌 변환은 무엇이 있을까?? 또, 일차변환 이후 편향만큼 평행이동하는 변환을 아핀변환이라 하는데 ($Y = AX+b$) 아핀변환을 단순히 하나의 행렬과의 곱으로 나타낼 방법은 없을까?? $\rightarrow$ 동차좌표계(Homogeneous Coordinate)


### 3. 조건부 확률 분포

MVN에 있어서 $P(\mathbf{x}_2 | \mathbf{x}_1)$은 $\mathbf{x}_1$과 무관하게 항상 MVN이다. 즉 다시말해서 조건부 확률 분포도 항상 MVN이라는 것입니다!! 

이는 총 $m$차원의 MVN에서 $n$차원의 $\mathbf{x}_1$ 조건부 분포를 구한다는 것은 원래의 분포에서 $n$차원 만큼을 덜어내는 것과 같습니다!! 그만큼의 차원을 고정시키는 것이죠!

예를 들어 이차원 결합 정규분포에 대해 1차원 $x_1=c$의 조건부 분포 $P(x_2 | x_1 = c)$는 하나의 차원을 $c$로 고정시킨 단일 정규분포가 되는 것이죠!!

쉬운 내용이니 넘어갈게요


### 4. 백색화

백색화는 앞서 설명드린 아핀 변환의 연장이라고 생각하면 될 것 같아요. 어떠한 데이터가 $D$차원 공간에서 찌그러진 초타원부피 모양의 분포를 갖는다고 가정해 봅시다. 이러한 데이터들을 분석하려 하는데 분포의 모양이 타원이면 분석하기 힘들겠죠?? 

그래서 각 변수들 사이 상관 관계가 존재하는 복잡한 분포를, 상관 관계가 제거된 단순한 표준 정규 분포(평균은 0이고, 분산은 $I$인 분포)로 바꾼 뒤 분석하는게 백색화의 아이디어입니다.

따라서 타원 모양의 찌그러진 복잡한 분포를 원(또는 구.. 또는 4차원 이상에서의 구) 모양으로 바꾼 뒤 분포의 중심(평균)을 0으로 옮기면 되지 않을까요??

뭔가 익숙하죠??, 네 맞습니다. 아핀변환을 적용하면 임의의 복잡한 분포를 단순한 분포로 바꾸어 분석할 수 있습니다.

앞서 등장한 식 (a) $x = \mu + Lz$를 다시 활용해 봅시다.

$$ \begin{align} x = \mu + Lz & \Rightarrow Lz = x - \mu \\
&\Rightarrow z = L^{-1}(x-\mu) \end{align}$$

이러한 연산을 통해 만든 새로운 분포 $L$은 기존 분포의 통계적 유의성을 어느정도 보존하면서 분석하기 쉬운 표준정규분포의 형태로 바꾸어 줍니다. 이때 백색화된 새로운 분포 상에서의 유클리드 거리는 마할라노비스 거리와 항상 같습니다!!

앞서, 마할라노비스 거리는 분포를 반영한 거리라고 말씀드렸습니다! 예를 들어 이차원 상에서 양의 상관관계를 갖는 데이터들이 제1사분면과 제3사분면에 직선 형태로 존재한다고 가정해 봅시다.

![마할라노비스](https://mblogthumb-phinf.pstatic.net/20100511_110/intencelove_1273586341677X5A5J_jpg/2_intencelove.jpg?type=w420)

분포를 반영하였기 때문에 유클리드 거리는 $AC$가 더 클지라도 마할라노비스 거리는 $AB$가 더 멉니다. 

> 사실 고윳값 분해와도 관련 있는 내용입니다! 해당 분포를 고윳값 분해했을 때 고유벡터의 방향과 가까운 벡터는 마할라노비스 거리가 작습니다

백색화를 진행하여 데이터의 분산이 $I$가 된 행렬은 서로의 상관관계가 제거되었으므로, 모든 데이터들의 마할라노비스 거리가 유클리디안 거리와 같습니다! 직관적으로 이해하면 이렇구요. 수식으로 이해하자면 아래와 같습니다.

* 원공간 상에서의 유클리드 거리 ${D_E}^2 = z^T z$

* 마할라노비스 거리 ${D_M}^2 = (x-\mu)^T \sum^{-1} (x-\mu)$

$${\sum}^{-1} = (LL^T)^{-1} = (L^T)^{-1}(L^{-1})$$
$$\begin{align} {D_M}^2 & = (x-\mu)^T {\sum}^{-1} (x- \mu) \\
& = (s- \mu)^T (L^T)^{-1}L^{-1}(x-\mu) \\
& = [L^{-1}(x-\mu)]^T [L^{-1}(x-\mu)] \\
& = z^T z = {D_E}^2
\end{align} $$


이상으로 자습서 내용은 모두 끝입니다! 
